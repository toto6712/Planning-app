import pandas as pd
import io
import re
from typing import List, Tuple
from ..models import Intervention, Intervenant
import logging
import chardet
from .csv_cleaner import clean_csv_file

logger = logging.getLogger(__name__)

def extract_city_from_address(address: str) -> str:
    """Extrait la ville depuis une adresse pour d√©finir le secteur"""
    try:
        # Nettoyer l'adresse
        address = address.strip()
        
        # Patterns courants pour extraire la ville
        # Ex: "12 rue des Vosges 67000 Strasbourg" -> "Strasbourg"
        # Ex: "8 avenue de la Paix, Strasbourg" -> "Strasbourg"
        
        # Essayer de d√©tecter via code postal + ville
        postal_city_pattern = r'\b\d{5}\s+([A-Z√Ä-√ø][a-z√†-√ø\-\s]+)$'
        match = re.search(postal_city_pattern, address, re.IGNORECASE)
        if match:
            return match.group(1).strip().title()
        
        # Essayer de d√©tecter ville apr√®s virgule
        comma_pattern = r',\s*([A-Z√Ä-√ø][a-z√†-√ø\-\s]+)$'
        match = re.search(comma_pattern, address, re.IGNORECASE)
        if match:
            return match.group(1).strip().title()
        
        # Derni√®re tentative : prendre les derniers mots
        words = address.split()
        if len(words) >= 2:
            # Prendre les 2 derniers mots si le dernier est une ville
            last_words = ' '.join(words[-2:])
            if not re.search(r'\d', last_words):  # Pas de chiffres
                return last_words.title()
            elif len(words) >= 1:
                # Sinon prendre juste le dernier mot
                last_word = words[-1]
                if not re.search(r'\d', last_word):
                    return last_word.title()
        
        # Par d√©faut, retourner une partie de l'adresse
        return "Secteur Inconnu"
        
    except Exception as e:
        logger.warning(f"Erreur extraction ville de '{address}': {str(e)}")
        return "Secteur Inconnu"

def detect_special_intervenants(nom_prenom: str) -> Tuple[List[str], str]:
    """D√©tecte les intervenants sp√©ciaux et leurs contraintes"""
    specialites = []
    plage_horaire = ""
    
    nom_lower = nom_prenom.lower()
    
    # D√©tecter les cas sp√©ciaux mentionn√©s dans le prompt
    if "castano" in nom_lower and "leslie" in nom_lower:
        specialites.append("14h-22h_only")
        plage_horaire = "14h00-22h00"
    elif "benamrouze" in nom_lower and "larbi" in nom_lower:
        specialites.append("volant")
    
    return specialites, plage_horaire

def calculate_weekend_roulement(heure_hebdomaire: str) -> str:
    """Calcule le roulement week-end selon les heures hebdomadaires"""
    try:
        # Extraire le nombre d'heures
        heures = int(re.findall(r'\d+', heure_hebdomaire)[0])
        
        if heures >= 35:
            # Les temps pleins sont en roulement A/B (√† attribuer automatiquement)
            return "A"  # Par d√©faut, l'IA d√©cidera du roulement
        else:
            return "exempt"
    except:
        return "exempt"

def detect_encoding(file_content: bytes) -> str:
    """D√©tecte l'encodage du fichier CSV"""
    try:
        # Essayer de d√©tecter l'encodage automatiquement
        detected = chardet.detect(file_content)
        encoding = detected.get('encoding', 'utf-8')
        logger.info(f"Encodage d√©tect√©: {encoding} (confiance: {detected.get('confidence', 0):.2f})")
        return encoding
    except Exception as e:
        logger.warning(f"Impossible de d√©tecter l'encodage: {str(e)}")
        return 'utf-8'

def try_multiple_encodings(file_content: bytes) -> pd.DataFrame:
    """Essaie plusieurs encodings et options pour lire le fichier CSV"""
    # D'abord, essayer de nettoyer le fichier
    try:
        logger.info("Tentative de nettoyage du fichier CSV")
        cleaned_content = clean_csv_file(file_content)
    except Exception as e:
        logger.warning(f"√âchec du nettoyage: {str(e)}")
        cleaned_content = file_content
    
    encodings_to_try = [
        'utf-8',
        'utf-8-sig',  # UTF-8 avec BOM
        'iso-8859-1',  # Latin-1
        'cp1252',  # Windows-1252
        'ascii'
    ]
    
    # Ajouter l'encodage d√©tect√© en premier
    detected_encoding = detect_encoding(cleaned_content)
    if detected_encoding and detected_encoding not in encodings_to_try:
        encodings_to_try.insert(0, detected_encoding)
    
    # Options de lecture CSV pour g√©rer diff√©rents cas
    csv_options_list = [
        {},  # Options par d√©faut
        {'sep': ';'},  # S√©parateur point-virgule
        {'quotechar': '"', 'quoting': 1},  # Gestion des guillemets
        {'skipinitialspace': True},  # Ignorer les espaces initiaux
        {'on_bad_lines': 'skip'},  # Ignorer les lignes malform√©es (pandas r√©cent)
    ]
    
    # Pour les versions plus anciennes de pandas
    try:
        import pandas as pd
        if hasattr(pd, '__version__') and int(pd.__version__.split('.')[0]) < 1:
            csv_options_list.append({'error_bad_lines': False, 'warn_bad_lines': True})
    except:
        pass
    
    for encoding in encodings_to_try:
        for options in csv_options_list:
            try:
                logger.info(f"Tentative avec encodage: {encoding}, options: {options}")
                
                df = pd.read_csv(io.BytesIO(cleaned_content), encoding=encoding, **options)
                
                # V√©rifier que le DataFrame est valide
                if not df.empty and len(df.columns) >= 3:
                    logger.info(f"‚úÖ Lecture r√©ussie avec {encoding} et options {options}")
                    return df
                else:
                    logger.warning(f"DataFrame invalide avec {encoding}")
                    
            except Exception as e:
                logger.debug(f"√âchec avec {encoding} et {options}: {str(e)}")
                continue
    
    # Derni√®re tentative avec nettoyage manuel ligne par ligne
    try:
        logger.info("Tentative de nettoyage manuel avanc√©")
        content_str = cleaned_content.decode('utf-8', errors='ignore')
        
        lines = content_str.split('\n')
        if not lines:
            raise ValueError("Fichier vide")
        
        # Analyser l'en-t√™te
        header = lines[0].strip()
        if not header:
            raise ValueError("En-t√™te manquant")
        
        expected_columns = len(header.split(','))
        logger.info(f"En-t√™te: {header}, colonnes attendues: {expected_columns}")
        
        # Construire un CSV valide
        valid_lines = [header]
        for i, line in enumerate(lines[1:], start=2):
            if line.strip():
                # Essayer de corriger la ligne
                parts = line.split(',')
                if len(parts) != expected_columns:
                    logger.warning(f"Ligne {i}: {len(parts)} champs au lieu de {expected_columns}")
                    # Correction simple: prendre les premi√®res colonnes et fusionner le reste
                    if len(parts) > expected_columns:
                        corrected_parts = parts[:expected_columns-1]
                        corrected_parts.append(','.join(parts[expected_columns-1:]))
                        line = ','.join(corrected_parts)
                
                valid_lines.append(line.strip())
        
        # Cr√©er le DataFrame √† partir du contenu corrig√©
        corrected_content = '\n'.join(valid_lines)
        df = pd.read_csv(io.StringIO(corrected_content))
        
        if not df.empty:
            logger.info("‚úÖ Lecture r√©ussie apr√®s correction manuelle")
            return df
            
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage manuel: {str(e)}")
    
    raise ValueError(
        "‚ùå Impossible de lire le fichier CSV. V√©rifiez que :\n"
        "1. Le fichier est bien au format CSV\n"
        "2. Les colonnes sont s√©par√©es par des virgules\n"
        "3. Il n'y a pas de virgules dans les donn√©es (utilisez des guillemets si n√©cessaire)\n"
        "4. Toutes les lignes ont le m√™me nombre de colonnes\n"
        "5. L'encodage est UTF-8"
    )

def parse_interventions_csv(file_content: bytes) -> List[Intervention]:
    """Parse le fichier interventions.csv et retourne une liste d'Intervention"""
    try:
        # Lire le CSV avec d√©tection d'encodage
        df = try_multiple_encodings(file_content)
        
        # Nettoyer les noms de colonnes (enlever les espaces et caract√®res invisibles)
        df.columns = df.columns.str.strip()
        
        # Supprimer les lignes compl√®tement vides
        df = df.dropna(how='all')
        
        # V√©rifier les colonnes obligatoires (avec latitude/longitude)
        required_columns = ['Client', 'Date', 'Duree', 'Latitude', 'Longitude']
        available_columns = df.columns.tolist()
        
        # Mapping flexible des colonnes
        column_mapping = {}
        for req_col in required_columns:
            found = False
            for avail_col in available_columns:
                # Comparaison flexible (sans accents et casse)
                req_normalized = req_col.lower().replace('√©', 'e').replace('√®', 'e').replace('√†', 'a').replace('√ß', 'c')
                avail_normalized = avail_col.lower().replace('√©', 'e').replace('√®', 'e').replace('√†', 'a').replace('√ß', 'c').replace('√£¬©', 'e').replace('√£¬®', 'e')
                
                if req_normalized == avail_normalized:
                    column_mapping[req_col] = avail_col
                    found = True
                    logger.info(f"Mapping flexible: {req_col} -> {avail_col}")
                    break
                    
            if not found:
                # Recherche partielle pour certains termes sp√©ciaux
                for avail_col in available_columns:
                    avail_lower = avail_col.lower().replace('√©', 'e').replace('√£¬©', 'e')
                    req_lower = req_col.lower().replace('√©', 'e')
                    
                    if (req_lower == 'duree' and ('duree' in avail_lower or 'duree' in avail_lower or 'duration' in avail_lower)) or \
                       (req_lower in avail_lower):
                        column_mapping[req_col] = avail_col
                        found = True
                        logger.info(f"Mapping partiel: {req_col} -> {avail_col}")
                        break
            if not found:
                raise ValueError(f"Colonne manquante dans interventions.csv: '{req_col}'. Colonnes disponibles: {available_columns}")
        
        # Rechercher la colonne Intervenant
        intervenant_col = None
        for col in available_columns:
            if 'intervenant' in col.lower():
                intervenant_col = col
                break
        
        logger.info(f"Colonnes mapp√©es: {column_mapping}")
        if intervenant_col:
            logger.info(f"Colonne intervenant trouv√©e: {intervenant_col}")
        
        logger.info(f"üìä PARSING INTERVENTIONS - {len(df)} lignes d√©tect√©es")
        
        interventions = []
        for index, row in df.iterrows():
            try:
                if (index + 1) % 5 == 0:  # Log tous les 5 lignes
                    logger.info(f"   üîÑ Traitement ligne {index + 1}/{len(df)}")
                
                logger.debug(f"Traitement ligne {index + 2}: {dict(row)}")
                
                # V√©rifier que les colonnes critiques ne sont pas vides
                client = str(row[column_mapping['Client']]).strip()
                date = str(row[column_mapping['Date']]).strip()
                duree = str(row[column_mapping['Duree']]).strip()
                latitude = row[column_mapping['Latitude']]
                longitude = row[column_mapping['Longitude']]
                
                logger.debug(f"Ligne {index + 2}: Valeurs extraites - client='{client}', date='{date}', duree='{duree}', lat='{latitude}', lon='{longitude}'")
                
                # Ignorer les lignes avec des valeurs manquantes critiques
                if (pd.isna(row[column_mapping['Client']]) or 
                    pd.isna(row[column_mapping['Date']]) or 
                    pd.isna(row[column_mapping['Duree']]) or
                    pd.isna(row[column_mapping['Latitude']]) or
                    pd.isna(row[column_mapping['Longitude']]) or
                    client.lower() in ['nan', ''] or 
                    date.lower() in ['nan', ''] or
                    duree.lower() in ['nan', '']):
                    logger.warning(f"   ‚ö†Ô∏è Ligne {index + 2} ignor√©e : donn√©es manquantes critiques")
                    continue
                
                # Valider les coordonn√©es
                try:
                    # G√©rer les virgules fran√ßaises comme s√©parateurs d√©cimaux
                    lat_str = str(latitude).replace(',', '.')
                    lon_str = str(longitude).replace(',', '.')
                    
                    lat = float(lat_str)
                    lon = float(lon_str)
                    
                    logger.debug(f"   ‚úÖ Ligne {index + 2}: Coordonn√©es valid√©es - lat={lat}, lon={lon}")
                    
                    # V√©rifier que les coordonn√©es sont valides
                    if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):
                        logger.warning(f"   ‚ùå Ligne {index + 2} ignor√©e : coordonn√©es invalides ({lat}, {lon})")
                        continue
                        
                except (ValueError, TypeError) as e:
                    logger.warning(f"   ‚ùå Ligne {index + 2} ignor√©e : coordonn√©es non num√©riques - latitude='{latitude}', longitude='{longitude}', error={str(e)}")
                    continue
                
                # R√©cup√©rer l'intervenant (peut √™tre vide)
                intervenant = ""
                if intervenant_col and pd.notna(row.get(intervenant_col)):
                    intervenant = str(row[intervenant_col]).strip()
                    if intervenant.lower() == 'nan':
                        intervenant = ""
                
                # D√©terminer le secteur (optionnel, peut √™tre d√©riv√© des coordonn√©es si n√©cessaire)
                secteur = ""  # Peut √™tre ajout√© plus tard via g√©ocodage inverse si n√©cessaire
                
                # D√©tecter si c'est une intervention bin√¥me (colonne optionnelle)
                binome = False
                if 'Binome' in df.columns or 'Bin√¥me' in df.columns:
                    binome_col = 'Binome' if 'Binome' in df.columns else 'Bin√¥me'
                    if pd.notna(row.get(binome_col)):
                        binome_val = str(row[binome_col]).strip().lower()
                        binome = binome_val in ['true', '1', 'oui', 'yes']
                
                # D√©tecter l'intervenant r√©f√©rent (colonne optionnelle)
                intervenant_referent = ""
                if 'Intervenant_referent' in df.columns or 'R√©f√©rent' in df.columns:
                    ref_col = 'Intervenant_referent' if 'Intervenant_referent' in df.columns else 'R√©f√©rent'
                    if pd.notna(row.get(ref_col)):
                        intervenant_referent = str(row[ref_col]).strip()
                        if intervenant_referent.lower() == 'nan':
                            intervenant_referent = ""

                intervention = Intervention(
                    client=client,
                    date=date,
                    duree=duree,
                    latitude=lat,
                    longitude=lon,
                    intervenant=intervenant,
                    binome=binome,
                    intervenant_referent=intervenant_referent,
                    secteur=secteur
                )
                interventions.append(intervention)
                
                logger.debug(f"Intervention cr√©√©e: {intervention.client} le {intervention.date} √† ({lat:.4f},{lon:.4f})")
                
            except Exception as e:
                logger.warning(f"Erreur ligne {index + 2}: {str(e)} - Ligne ignor√©e")
                continue
        
        if not interventions:
            raise ValueError("Aucune intervention valide trouv√©e dans le fichier")
        
        logger.info(f"‚úÖ PARSING INTERVENTIONS TERMIN√â")
        logger.info(f"   ‚Ä¢ Lignes trait√©es: {len(df)}")
        logger.info(f"   ‚Ä¢ Interventions valides: {len(interventions)}")
        logger.info(f"   ‚Ä¢ Taux de succ√®s: {len(interventions)/len(df)*100:.1f}%")
        return interventions
        
    except Exception as e:
        logger.error(f"Erreur parsing interventions.csv: {str(e)}")
        raise ValueError(f"Erreur lors du parsing des interventions: {str(e)}")

def parse_intervenants_csv(file_content: bytes) -> List[Intervenant]:
    """Parse le fichier intervenants.csv et retourne une liste d'Intervenant"""
    try:
        # Lire le CSV avec d√©tection d'encodage
        df = try_multiple_encodings(file_content)
        
        # Nettoyer les noms de colonnes
        df.columns = df.columns.str.strip()
        
        # Supprimer les lignes compl√®tement vides
        df = df.dropna(how='all')
        
        # V√©rifier les colonnes obligatoires (avec latitude/longitude)
        required_columns = ['Nom_Prenom', 'Latitude', 'Longitude', 'Heure_Mensuel', 'Heure_hebdomadaire']
        available_columns = df.columns.tolist()
        
        logger.info(f"Colonnes requises: {required_columns}")
        logger.info(f"Colonnes disponibles: {available_columns}")
        
        # Mapping flexible des colonnes (insensible √† la casse)
        column_mapping = {}
        for req_col in required_columns:
            found = False
            for avail_col in available_columns:
                # Comparaison directe d'abord
                if req_col == avail_col:
                    column_mapping[req_col] = avail_col
                    found = True
                    logger.info(f"Mapping direct: {req_col} -> {avail_col}")
                    break
                # Comparaison insensible √† la casse
                elif req_col.lower() == avail_col.lower():
                    column_mapping[req_col] = avail_col
                    found = True
                    logger.info(f"Mapping insensible √† la casse: {req_col} -> {avail_col}")
                    break
                # Comparaison flexible (sans espaces, underscores, accents)
                req_normalized = req_col.lower().replace('√©', 'e').replace('√®', 'e').replace('_', '').replace(' ', '')
                avail_normalized = avail_col.lower().replace('√©', 'e').replace('√®', 'e').replace('_', '').replace(' ', '')
                if req_normalized == avail_normalized:
                    column_mapping[req_col] = avail_col
                    found = True
                    logger.info(f"Mapping flexible: {req_col} -> {avail_col}")
                    break
            if not found:
                # Recherche partielle pour compatibilit√©
                for avail_col in available_columns:
                    avail_lower = avail_col.lower().replace('√©', 'e').replace(' ', '').replace('_', '')
                    req_lower = req_col.lower().replace('√©', 'e').replace('_', '').replace(' ', '')
                    
                    # Mapping sp√©cial pour compatibilit√©
                    if 'nom' in req_lower and ('nom' in avail_lower or 'prenom' in avail_lower):
                        column_mapping[req_col] = avail_col
                        found = True
                        logger.info(f"Mapping nom: {req_col} -> {avail_col}")
                        break
                    elif 'latitude' in req_lower and 'latitude' in avail_lower:
                        column_mapping[req_col] = avail_col
                        found = True
                        logger.info(f"Mapping latitude: {req_col} -> {avail_col}")
                        break
                    elif 'longitude' in req_lower and 'longitude' in avail_lower:
                        column_mapping[req_col] = avail_col
                        found = True
                        logger.info(f"Mapping longitude: {req_col} -> {avail_col}")
                        break
                    elif 'heure' in req_lower and 'heure' in avail_lower:
                        if ('mensuel' in req_lower and 'mensuel' in avail_lower) or ('hebdomaire' in req_lower and 'hebdomaire' in avail_lower):
                            column_mapping[req_col] = avail_col
                            found = True
                            logger.info(f"Mapping heure: {req_col} -> {avail_col}")
                            break
                    elif req_lower in avail_lower:
                        column_mapping[req_col] = avail_col
                        found = True
                        logger.info(f"Mapping partiel: {req_col} -> {avail_col}")
                        break
            if not found:
                logger.error(f"Colonne '{req_col}' non trouv√©e parmi {available_columns}")
                raise ValueError(f"Colonne manquante dans intervenants.csv: '{req_col}'. Colonnes disponibles: {available_columns}")
        
        logger.info(f"Colonnes mapp√©es: {column_mapping}")
        
        logger.info(f"üìä PARSING INTERVENANTS - {len(df)} lignes d√©tect√©es")
        
        intervenants = []
        noms_vus = set()  # Pour d√©tecter les doublons
        
        for index, row in df.iterrows():
            try:
                if (index + 1) % 3 == 0:  # Log tous les 3 lignes
                    logger.info(f"   üîÑ Traitement ligne {index + 1}/{len(df)}")
                    
                # V√©rifier que les colonnes critiques ne sont pas vides
                nom = str(row[column_mapping['Nom_Prenom']]).strip()
                latitude = row[column_mapping['Latitude']]
                longitude = row[column_mapping['Longitude']]
                temps_mensuel = str(row[column_mapping['Heure_Mensuel']]).strip()
                temps_hebdo = str(row[column_mapping['Heure_hebdomadaire']]).strip()
                
                # Ignorer les lignes avec des valeurs manquantes critiques
                if (pd.isna(row[column_mapping['Nom_Prenom']]) or 
                    pd.isna(row[column_mapping['Latitude']]) or
                    pd.isna(row[column_mapping['Longitude']]) or 
                    pd.isna(row[column_mapping['Heure_Mensuel']]) or
                    pd.isna(row[column_mapping['Heure_hebdomadaire']]) or
                    nom.lower() in ['nan', ''] or 
                    temps_mensuel.lower() in ['nan', ''] or
                    temps_hebdo.lower() in ['nan', '']):
                    logger.warning(f"   ‚ö†Ô∏è Ligne {index + 2} ignor√©e : donn√©es manquantes critiques")
                    continue
                
                # Valider les coordonn√©es
                try:
                    # G√©rer les virgules fran√ßaises comme s√©parateurs d√©cimaux
                    lat_str = str(latitude).replace(',', '.')
                    lon_str = str(longitude).replace(',', '.')
                    
                    lat = float(lat_str)
                    lon = float(lon_str)
                    
                    logger.debug(f"   ‚úÖ Ligne {index + 2}: Coordonn√©es valid√©es - lat={lat}, lon={lon}")
                    
                    # V√©rifier que les coordonn√©es sont valides
                    if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):
                        logger.warning(f"   ‚ùå Ligne {index + 2} ignor√©e : coordonn√©es invalides ({lat}, {lon})")
                        continue
                        
                except (ValueError, TypeError) as e:
                    logger.warning(f"   ‚ùå Ligne {index + 2} ignor√©e : coordonn√©es non num√©riques - latitude='{latitude}', longitude='{longitude}', error={str(e)}")
                    continue
                
                # V√©rifier les doublons par nom (insensible √† la casse)
                nom_lower = nom.lower()
                if nom_lower in noms_vus:
                    logger.warning(f"   üîÑ Ligne {index + 2} ignor√©e : intervenant '{nom}' d√©j√† pr√©sent (doublon d√©tect√©)")
                    continue
                
                # D√©tecter les sp√©cialit√©s et plages horaires sp√©ciales
                specialites, plage_horaire_autorisee = detect_special_intervenants(nom)
                
                # Calculer le roulement week-end selon les heures
                roulement_weekend = calculate_weekend_roulement(temps_hebdo)
                
                intervenant = Intervenant(
                    nom_prenom=nom,
                    latitude=lat,
                    longitude=lon,
                    heure_hebdomaire=temps_hebdo,
                    heure_mensuel=temps_mensuel,
                    plage_horaire_autorisee=plage_horaire_autorisee,
                    specialites=specialites,
                    roulement_weekend=roulement_weekend
                )
                intervenants.append(intervenant)
                noms_vus.add(nom_lower)
                
                logger.debug(f"Intervenant cr√©√©: {intervenant.nom_prenom} √† ({lat:.4f},{lon:.4f})")
                
            except Exception as e:
                logger.warning(f"Erreur ligne {index + 2}: {str(e)} - Ligne ignor√©e")
                continue
        
        if not intervenants:
            raise ValueError("Aucun intervenant valide trouv√© dans le fichier")
        
        logger.info(f"Parsed {len(intervenants)} intervenants valides")
        return intervenants
        
    except Exception as e:
        logger.error(f"Erreur parsing intervenants.csv: {str(e)}")
        raise ValueError(f"Erreur lors du parsing des intervenants: {str(e)}")

def validate_csv_data(interventions: List[Intervention], intervenants: List[Intervenant]) -> Tuple[bool, str]:
    """Valide la coh√©rence des donn√©es CSV"""
    try:
        # V√©rifier qu'il y a des donn√©es
        if not interventions:
            return False, "Aucune intervention trouv√©e"
        
        if not intervenants:
            return False, "Aucun intervenant trouv√©"
        
        # V√©rifier les doublons dans les intervenants
        noms_intervenants = [i.nom_prenom for i in intervenants]
        noms_uniques = set(noms_intervenants)
        if len(noms_intervenants) != len(noms_uniques):
            doublons = [nom for nom in noms_uniques if noms_intervenants.count(nom) > 1]
            logger.warning(f"Doublons d√©tect√©s dans les intervenants : {doublons}")
            return False, f"Doublons d√©tect√©s dans les intervenants : {', '.join(doublons)}"
        
        # V√©rifier la coh√©rence des intervenants impos√©s
        intervenant_names = {i.nom_prenom for i in intervenants}
        for intervention in interventions:
            if intervention.intervenant and intervention.intervenant not in intervenant_names:
                logger.warning(f"Intervenant '{intervention.intervenant}' non trouv√© dans la liste des intervenants")
        
        # V√©rifier les formats de date
        for intervention in interventions:
            try:
                # V√©rifier le format de date "29/06/2025 08:00"
                if not intervention.date or len(intervention.date.split()) != 2:
                    return False, f"Format de date invalide pour {intervention.client}: {intervention.date}"
            except:
                return False, f"Erreur dans la date pour {intervention.client}"
        
        logger.info("Validation CSV r√©ussie")
        return True, "Donn√©es CSV valides"
        
    except Exception as e:
        logger.error(f"Erreur validation CSV: {str(e)}")
        return False, f"Erreur de validation: {str(e)}"